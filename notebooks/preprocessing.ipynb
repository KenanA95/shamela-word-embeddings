{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(input_file, stop_words, output_file):\n",
    "    out = open(output_file, 'w', encoding='utf-8')\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.split()\n",
    "            filtered_sentence = [w.lower().strip() for w in words if not w in stop_words]\n",
    "            out.write(\" \".join(filtered_sentence) + \"\\n\")\n",
    "    out.close()\n",
    "\n",
    "directory = \"data/compiled/\"\n",
    "sw = read_stop_words(\"arabic_stopwords_compiled.txt\")\n",
    "print(\"Stop words: \" + str(sw))\n",
    "\n",
    "for century in list(range(100, 1600, 100)):\n",
    "    corpus_name = directory + \"corpus_\" + str(century) + \"AH\"\n",
    "    output_name = directory + \"corpus_\" + str(century) + \"AH_nonstop\"\n",
    "    print(\"Starting on corpus \", corpus_name)\n",
    "    remove_stop_words(corpus_name, sw, output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\Kenan\\\\stanfordnlp_resources\\\\ar_padt_models\\\\ar_padt_tokenizer.pt', 'lang': 'ar', 'shorthand': 'ar_padt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\Kenan\\\\stanfordnlp_resources\\\\ar_padt_models\\\\ar_padt_lemmatizer.pt', 'lang': 'ar', 'shorthand': 'ar_padt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['إسماعيل']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "nlp = stanfordnlp.Pipeline(lang='ar', processors=\"tokenize,lemma\")\n",
    "doc = nlp(\"إسماعيل\")\n",
    "for sentence in doc.sentences:\n",
    "    lemmatized_sentence = [word.lemma for word in sentence.words]\n",
    "lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\Kenan\\\\stanfordnlp_resources\\\\ar_padt_models\\\\ar_padt_tokenizer.pt', 'lang': 'ar', 'shorthand': 'ar_padt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\Kenan\\\\stanfordnlp_resources\\\\ar_padt_models\\\\ar_padt_lemmatizer.pt', 'lang': 'ar', 'shorthand': 'ar_padt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n",
      "Completed 1000 lines\n",
      "Completed 2000 lines\n",
      "Completed 3000 lines\n",
      "Completed 4000 lines\n",
      "Completed 5000 lines\n",
      "Completed 6000 lines\n",
      "Completed 7000 lines\n",
      "Completed 8000 lines\n",
      "Completed 9000 lines\n",
      "Completed 10000 lines\n",
      "Completed 11000 lines\n",
      "Completed 12000 lines\n",
      "Completed 13000 lines\n",
      "Completed 14000 lines\n",
      "Completed 15000 lines\n",
      "Completed 16000 lines\n",
      "Completed 17000 lines\n",
      "Completed 18000 lines\n",
      "Completed 19000 lines\n",
      "Completed 20000 lines\n",
      "Completed 21000 lines\n",
      "Completed 22000 lines\n",
      "Completed 23000 lines\n",
      "Completed 24000 lines\n",
      "Completed 25000 lines\n",
      "Completed 26000 lines\n",
      "Completed 27000 lines\n",
      "Completed 28000 lines\n",
      "Completed 29000 lines\n",
      "Completed 30000 lines\n",
      "Completed 31000 lines\n",
      "Completed 32000 lines\n",
      "Completed 33000 lines\n",
      "Completed 34000 lines\n",
      "Completed 35000 lines\n",
      "Completed 36000 lines\n",
      "Completed 37000 lines\n",
      "Completed 38000 lines\n",
      "Completed 39000 lines\n",
      "Completed 40000 lines\n",
      "Completed 41000 lines\n",
      "Completed 42000 lines\n",
      "Completed 43000 lines\n",
      "Completed 44000 lines\n",
      "Completed 45000 lines\n",
      "Completed 46000 lines\n",
      "Completed 47000 lines\n",
      "Completed 48000 lines\n",
      "Completed 49000 lines\n",
      "Completed 50000 lines\n",
      "Completed 51000 lines\n",
      "Completed 52000 lines\n",
      "Completed 53000 lines\n",
      "Completed 54000 lines\n",
      "Completed 55000 lines\n",
      "Completed 56000 lines\n",
      "Completed 57000 lines\n",
      "Completed 58000 lines\n",
      "Completed 59000 lines\n",
      "Completed 60000 lines\n",
      "Completed 61000 lines\n",
      "Completed 62000 lines\n",
      "Completed 63000 lines\n",
      "Completed 64000 lines\n",
      "Completed 65000 lines\n",
      "Completed 66000 lines\n",
      "Completed 67000 lines\n",
      "Completed 68000 lines\n",
      "Completed 69000 lines\n",
      "Completed 70000 lines\n",
      "Completed 71000 lines\n",
      "Completed 72000 lines\n",
      "Completed 73000 lines\n",
      "Completed 74000 lines\n",
      "Completed 75000 lines\n",
      "Completed 76000 lines\n",
      "Completed 77000 lines\n",
      "Completed 78000 lines\n",
      "Completed 79000 lines\n",
      "Completed 80000 lines\n",
      "Completed 81000 lines\n",
      "Completed 82000 lines\n",
      "Completed 83000 lines\n",
      "Completed 84000 lines\n",
      "Completed 85000 lines\n",
      "Completed 86000 lines\n",
      "Completed 87000 lines\n",
      "Completed 88000 lines\n",
      "Completed 89000 lines\n",
      "Completed 90000 lines\n",
      "Completed 91000 lines\n",
      "Completed 92000 lines\n",
      "Completed 93000 lines\n",
      "Completed 94000 lines\n",
      "Completed 95000 lines\n",
      "Completed 96000 lines\n",
      "Completed 97000 lines\n",
      "Completed 98000 lines\n",
      "Completed 99000 lines\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "    \n",
    "def apply_lemmatization(corpus_name, output_file):\n",
    "    nlp = stanfordnlp.Pipeline(lang='ar', processors=\"tokenize,lemma\")\n",
    "    out = open(output_file, 'w', encoding='utf-8')\n",
    "    count = 0\n",
    "    \n",
    "    with open(corpus_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        count = 0\n",
    "        block = []\n",
    "        for line in f:\n",
    "            block.extend(line.strip().split())\n",
    "            count += 1\n",
    "            if count % 10 == 0:\n",
    "                if len(block) > 1:\n",
    "                    doc = nlp(\" \".join(block))\n",
    "                    for sentence in doc.sentences:\n",
    "                        lemmatized_sentence = [word.lemma for word in sentence.words]\n",
    "                    out.write(\" \".join(lemmatized_sentence) + \"\\n\")\n",
    "                    block = []\n",
    "             \n",
    "            if count % 1000 == 0:\n",
    "                print(\"Completed \" + str(count) + \" lines\")\n",
    "            \n",
    "    out.close()\n",
    "        \n",
    "\n",
    "directory = 'data/compiled/'\n",
    "century = 100\n",
    "corpus_name = directory + \"corpus_\" + str(century) + \"AH_nonstop\"\n",
    "output_name = directory + \"corpus_\" + str(century) + \"AH_lemmatized\"\n",
    "apply_lemmatization(corpus_name, output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "\n",
    "def read_corpus(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield simple_preprocess(line)\n",
    "\n",
    "century=700\n",
    "filename = \"data/compiled/corpus_\" + str(century) + \"AH_lemmatized_clean\"\n",
    "out_file = \"data/gensim/corpus_\" + str(century) + \"AH_lemmatized.model\"\n",
    "print(\"Starting on corpus: \" + filename)\n",
    "# sentences = list(read_corpus(filename))\n",
    "model_700 = Word2Vec(sentences, size=300, window=8, min_count=10, workers=4)\n",
    "# print(list(model.wv.vocab))\n",
    "print(\"Top 3 closest to شتى \", model_700.wv.most_similar(\"شتى\", topn=10))\n",
    "# print(\"Top 3 closest to بَيت \", model.wv.most_similar(\"قَوم\", topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
