{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 11134 422928\n",
      "200 65185 8316454\n",
      "300 167853 48082376\n",
      "400 249809 77664268\n",
      "500 255145 87616491\n",
      "600 239393 72142021\n",
      "700 254776 80880102\n",
      "800 328867 136060133\n",
      "900 242250 90509565\n",
      "1000 198006 63720377\n",
      "1100 172178 40326850\n",
      "1200 166241 40838334\n",
      "1300 227917 65642557\n",
      "1400 196942 46592419\n",
      "1500 131134 31051510\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def corpus_total_and_unique_tokens(corpus_filename, min_count=10):\n",
    "    freq = Counter()\n",
    "\n",
    "    # Avoid loading the whole corpus into memory\n",
    "    with open(corpus_filename, 'r', encoding='utf-8') as f:\n",
    "        [freq.update(line.split()) for line in f]\n",
    "\n",
    "    min_count_dict = {key: val for key, val in freq.items() if val >= min_count}\n",
    "    return len(min_count_dict.items()), sum(min_count_dict.values())\n",
    "\n",
    "for century in list(range(100, 1600, 100)):\n",
    "    filename = \"data/compiled/corpus_\" + str(century) + \"AH_nonstop\"\n",
    "    unique, total = corpus_total_and_unique_tokens(filename)\n",
    "    print(century, unique, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extinct Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV Files with Token Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "        \n",
    "def corpus_word_frequencies_to_file(corpus_filename, out_file, min_count=10):\n",
    "    freq = Counter()\n",
    "\n",
    "    # Avoid loading the whole corpus into memory\n",
    "    with open(corpus_filename, 'r', encoding='utf-8') as f:\n",
    "        [freq.update(line.split()) for line in f]\n",
    "\n",
    "    with open(out_file, 'w', encoding='utf-8') as corpus_summary:\n",
    "        fieldnames = ['word', 'frequency']\n",
    "        writer = csv.DictWriter(corpus_summary, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for k, v in OrderedDict(freq.most_common()).items():\n",
    "            if v >= min_count:\n",
    "                writer.writerow({\"word\": k, \"frequency\": v})\n",
    "        print(out_file + \" contains: + \" + str(len(freq)) + \" unique tokens\")\n",
    "        \n",
    "for century in list(range(100, 1600, 100)):\n",
    "    filename = \"data/compiled/corpus_\" + str(century) + \"AH_nonstop\"\n",
    "    out_file = \"data/summary/corpus_\" + str(century) + \"AH_summary.csv\"\n",
    "    print(\"Starting on file name: \" + filename)\n",
    "    corpus_word_frequencies_to_file(filename, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the Tokens and Frequencies into Dataframes/Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "directory = \"data/summary/\"\n",
    "\n",
    "df_100AH =  pd.read_csv(directory + \"corpus_100AH_summary.csv\")\n",
    "df_200AH =  pd.read_csv(directory + \"corpus_200AH_summary.csv\")\n",
    "df_300AH =  pd.read_csv(directory + \"corpus_300AH_summary.csv\")\n",
    "df_400AH =  pd.read_csv(directory + \"corpus_400AH_summary.csv\")\n",
    "df_500AH =  pd.read_csv(directory + \"corpus_500AH_summary.csv\")\n",
    "df_600AH =  pd.read_csv(directory + \"corpus_600AH_summary.csv\")\n",
    "df_700AH =  pd.read_csv(directory + \"corpus_700AH_summary.csv\")\n",
    "df_800AH =  pd.read_csv(directory + \"corpus_800AH_summary.csv\")\n",
    "df_900AH =  pd.read_csv(directory + \"corpus_900AH_summary.csv\")\n",
    "df_1000AH = pd.read_csv(directory + \"corpus_1000AH_summary.csv\")\n",
    "df_1100AH = pd.read_csv(directory + \"corpus_1100AH_summary.csv\")\n",
    "df_1200AH = pd.read_csv(directory + \"corpus_1200AH_summary.csv\")\n",
    "df_1300AH = pd.read_csv(directory + \"corpus_1300AH_summary.csv\")\n",
    "df_1400AH = pd.read_csv(directory + \"corpus_1400AH_summary.csv\")\n",
    "df_1500AH = pd.read_csv(directory + \"corpus_1500AH_summary.csv\")\n",
    "\n",
    "set_100AH =  set(df_100AH.word)\n",
    "set_200AH =  set(df_200AH.word)\n",
    "set_300AH =  set(df_300AH.word)\n",
    "set_400AH =  set(df_400AH.word)\n",
    "set_500AH =  set(df_500AH.word)\n",
    "set_600AH =  set(df_600AH.word)\n",
    "set_700AH =  set(df_700AH.word)\n",
    "set_800AH =  set(df_800AH.word)\n",
    "set_900AH =  set(df_900AH.word)\n",
    "set_1000AH = set(df_1000AH.word)\n",
    "set_1100AH = set(df_1100AH.word)\n",
    "set_1200AH = set(df_1200AH.word)\n",
    "set_1300AH = set(df_1300AH.word)\n",
    "set_1400AH = set(df_1400AH.word)\n",
    "set_1500AH = set(df_1500AH.word)\n",
    "\n",
    "all_sets = [set_100AH, set_200AH, set_300AH, set_400AH, set_500AH, set_600AH, set_700AH, set_800AH, set_900AH, set_1000AH, \n",
    "            set_1100AH, set_1200AH, set_1300AH, set_1400AH, set_1500AH]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the extinct words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_extinct_words(starting_list, century_start):\n",
    "    for word in list(starting_list):\n",
    "        for s in all_sets[century_start:]:\n",
    "            if word in s:\n",
    "                starting_list.discard(word)\n",
    "    return starting_list\n",
    "\n",
    "# Starting list is words that appeared in a century but not the subsequent century\n",
    "extinct_100AH =  find_extinct_words(set.difference(set_100AH, set_200AH), 2)\n",
    "extinct_200AH =  find_extinct_words(set.difference(set_200AH, set_300AH), 3)\n",
    "extinct_300AH =  find_extinct_words(set.difference(set_300AH, set_400AH), 4)\n",
    "extinct_400AH =  find_extinct_words(set.difference(set_400AH, set_500AH), 5)\n",
    "extinct_500AH =  find_extinct_words(set.difference(set_500AH, set_600AH), 6)\n",
    "extinct_600AH =  find_extinct_words(set.difference(set_600AH, set_700AH), 7)\n",
    "extinct_700AH =  find_extinct_words(set.difference(set_700AH, set_800AH), 8)\n",
    "extinct_800AH =  find_extinct_words(set.difference(set_800AH, set_900AH), 9)\n",
    "extinct_900AH =  find_extinct_words(set.difference(set_900AH, set_1000AH), 10)\n",
    "extinct_1000AH = find_extinct_words(set.difference(set_1000AH, set_1100AH), 11)\n",
    "extinct_1100AH = find_extinct_words(set.difference(set_1100AH, set_1200AH), 12)\n",
    "extinct_1200AH = find_extinct_words(set.difference(set_1200AH, set_1300AH), 13)\n",
    "extinct_1300AH = find_extinct_words(set.difference(set_1300AH, set_1400AH), 14)\n",
    "\n",
    "df_extinct_100AH = df_100AH[df_100AH.word.isin(list(extinct_100AH))]\n",
    "df_extinct_200AH = df_200AH[df_200AH.word.isin(list(extinct_200AH))]\n",
    "df_extinct_300AH = df_300AH[df_300AH.word.isin(list(extinct_300AH))]\n",
    "df_extinct_400AH = df_400AH[df_400AH.word.isin(list(extinct_400AH))]\n",
    "df_extinct_500AH = df_500AH[df_500AH.word.isin(list(extinct_500AH))]\n",
    "df_extinct_600AH = df_600AH[df_600AH.word.isin(list(extinct_600AH))]\n",
    "df_extinct_700AH = df_700AH[df_700AH.word.isin(list(extinct_700AH))]\n",
    "df_extinct_800AH = df_800AH[df_800AH.word.isin(list(extinct_800AH))]\n",
    "df_extinct_900AH = df_900AH[df_900AH.word.isin(list(extinct_900AH))]\n",
    "df_extinct_1000AH = df_1000AH[df_1000AH.word.isin(list(extinct_1000AH))]\n",
    "df_extinct_1100AH = df_1100AH[df_1100AH.word.isin(list(extinct_1100AH))]\n",
    "df_extinct_1200AH = df_1200AH[df_1200AH.word.isin(list(extinct_1200AH))]\n",
    "df_extinct_1300AH = df_1300AH[df_1300AH.word.isin(list(extinct_1300AH))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all results and export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>century</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>قرز</td>\n",
       "      <td>9204</td>\n",
       "      <td>900AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>علاى</td>\n",
       "      <td>8337</td>\n",
       "      <td>1000AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>طح</td>\n",
       "      <td>7185</td>\n",
       "      <td>900AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>إلاى</td>\n",
       "      <td>6154</td>\n",
       "      <td>1000AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>وبالأصل</td>\n",
       "      <td>4283</td>\n",
       "      <td>1300AH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  frequency century\n",
       "1381      قرز       9204   900AH\n",
       "1103     علاى       8337  1000AH\n",
       "1796       طح       7185   900AH\n",
       "1530     إلاى       6154  1000AH\n",
       "2341  وبالأصل       4283  1300AH"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "# Add a column to each dataframe indicating which century it came from\n",
    "df_extinct_100AH['century'] = '100AH'\n",
    "df_extinct_200AH['century'] = '200AH'\n",
    "df_extinct_300AH['century'] = '300AH'\n",
    "df_extinct_400AH['century'] = '400AH'\n",
    "df_extinct_500AH['century'] = '500AH'\n",
    "df_extinct_600AH['century'] = '600AH'\n",
    "df_extinct_700AH['century'] = '700AH'\n",
    "df_extinct_800AH['century'] = '800AH'\n",
    "df_extinct_900AH['century'] = '900AH'\n",
    "df_extinct_1000AH['century'] = '1000AH'\n",
    "df_extinct_1100AH['century'] = '1100AH'\n",
    "df_extinct_1200AH['century'] = '1200AH'\n",
    "df_extinct_1300AH['century'] = '1300AH'\n",
    "\n",
    "result = pd.concat([df_extinct_100AH, df_extinct_200AH, df_extinct_300AH, df_extinct_400AH, \n",
    "                    df_extinct_500AH, df_extinct_600AH, df_extinct_700AH, df_extinct_800AH,\n",
    "                    df_extinct_900AH, df_extinct_1000AH, df_extinct_1100AH, df_extinct_1200AH, df_extinct_1300AH])\n",
    "# Export to CSV\n",
    "result = result.sort_values(by=['frequency'], ascending=False)\n",
    "\n",
    "result.to_csv(\"data/extinct_words.csv\", encoding=\"utf-8\")\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>century</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>نفساه</td>\n",
       "      <td>35</td>\n",
       "      <td>100AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4318</th>\n",
       "      <td>أتحفظه</td>\n",
       "      <td>272</td>\n",
       "      <td>200AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17956</th>\n",
       "      <td>هربيس</td>\n",
       "      <td>228</td>\n",
       "      <td>300AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14945</th>\n",
       "      <td>واللوحة</td>\n",
       "      <td>514</td>\n",
       "      <td>400AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7018</th>\n",
       "      <td>دبيثى</td>\n",
       "      <td>1543</td>\n",
       "      <td>500AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>وخع</td>\n",
       "      <td>3008</td>\n",
       "      <td>600AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18749</th>\n",
       "      <td>بدليلهما</td>\n",
       "      <td>482</td>\n",
       "      <td>700AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13160</th>\n",
       "      <td>ورخه</td>\n",
       "      <td>1225</td>\n",
       "      <td>800AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>قرز</td>\n",
       "      <td>9204</td>\n",
       "      <td>900AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>علاى</td>\n",
       "      <td>8337</td>\n",
       "      <td>1000AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11083</th>\n",
       "      <td>تحتيته</td>\n",
       "      <td>494</td>\n",
       "      <td>1100AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8915</th>\n",
       "      <td>ورشيدي</td>\n",
       "      <td>601</td>\n",
       "      <td>1200AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>وبالأصل</td>\n",
       "      <td>4283</td>\n",
       "      <td>1300AH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  frequency century\n",
       "2497      نفساه         35   100AH\n",
       "4318     أتحفظه        272   200AH\n",
       "17956     هربيس        228   300AH\n",
       "14945   واللوحة        514   400AH\n",
       "7018      دبيثى       1543   500AH\n",
       "3140        وخع       3008   600AH\n",
       "18749  بدليلهما        482   700AH\n",
       "13160      ورخه       1225   800AH\n",
       "1381        قرز       9204   900AH\n",
       "1103       علاى       8337  1000AH\n",
       "11083    تحتيته        494  1100AH\n",
       "8915     ورشيدي        601  1200AH\n",
       "2341    وبالأصل       4283  1300AH"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 3 results only\n",
    "result_summary = pd.concat([df_extinct_100AH[:1], df_extinct_200AH[:1], df_extinct_300AH[:1], df_extinct_400AH[:1], \n",
    "                    df_extinct_500AH[:1], df_extinct_600AH[:1], df_extinct_700AH[:1], df_extinct_800AH[:1],\n",
    "                    df_extinct_900AH[:1], df_extinct_1000AH[:1], df_extinct_1100AH[:1], df_extinct_1200AH[:1], df_extinct_1300AH[:1]])\n",
    "\n",
    "# result_summary.to_csv(\"data/extinct_words_top3.csv\", encoding=\"utf-8\")\n",
    "result_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Words Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "\n",
    "# Grab the top N results from each word/frequency document\n",
    "def get_most_frequent(file_name, N=2):\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        for row in itertools.islice(csv.DictReader(f), N):\n",
    "            yield row\n",
    "            \n",
    "directory = \"data/summary/\"\n",
    "output_file = \"data/summary/popular_words.csv\"\n",
    "\n",
    "\n",
    "out = open(output_file, \"w\", encoding='utf-8', newline=\"\")\n",
    "writer = csv.DictWriter(out, fieldnames=[\"Word\", \"Frequency\", \"Century (AH)\"])\n",
    "writer.writeheader()\n",
    "\n",
    "for century in list(range(100, 1600, 200)):\n",
    "    corpus_name = \"corpus_\" + str(century) + \"AH_summary.csv\"\n",
    "    for res in get_most_frequent(directory + corpus_name):\n",
    "        writer.writerow({\"Word\": res['word'], \"Frequency\": res['frequency'], \"Century (AH)\": century})\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Century (AH)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الله</td>\n",
       "      <td>10975</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>السلام</td>\n",
       "      <td>3203</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الله</td>\n",
       "      <td>2259572</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>حدثنا</td>\n",
       "      <td>894564</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الله</td>\n",
       "      <td>2642254</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>أبو</td>\n",
       "      <td>1021880</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>الله</td>\n",
       "      <td>1846765</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>أبو</td>\n",
       "      <td>524548</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>الله</td>\n",
       "      <td>2225896</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>أبي</td>\n",
       "      <td>821922</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>الله</td>\n",
       "      <td>647521</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>قوله</td>\n",
       "      <td>244802</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>الله</td>\n",
       "      <td>818533</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>قوله</td>\n",
       "      <td>655657</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>الله</td>\n",
       "      <td>913519</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>عبد</td>\n",
       "      <td>245418</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word  Frequency  Century (AH)\n",
       "0     الله      10975           100\n",
       "1   السلام       3203           100\n",
       "2     الله    2259572           300\n",
       "3    حدثنا     894564           300\n",
       "4     الله    2642254           500\n",
       "5      أبو    1021880           500\n",
       "6     الله    1846765           700\n",
       "7      أبو     524548           700\n",
       "8     الله    2225896           900\n",
       "9      أبي     821922           900\n",
       "10    الله     647521          1100\n",
       "11    قوله     244802          1100\n",
       "12    الله     818533          1300\n",
       "13    قوله     655657          1300\n",
       "14    الله     913519          1500\n",
       "15     عبد     245418          1500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/summary/popular_words.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
